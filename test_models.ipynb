{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on \n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/\n",
    "\n",
    "https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from custom_tensorboard import TrainValTensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = dstack(loaded)\n",
    "\treturn loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix)\n",
    "    #trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix)\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = load_dataset('../data/HAR/UCI_HAR_Dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 128, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Lambda, Input, Dropout, Flatten, LSTM, Concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cb = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, min_delta = 0.01, patience = 3, verbose = 1)\n",
    "es_cb = EarlyStopping(monitor = 'val_loss', min_delta=0.01, patience = 10, verbose = 1, restore_best_weights = True)\n",
    "callbacks = [lr_cb, es_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.adam(lr=0.01)\n",
    "validation_split_on_training = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_model_generator(n_timesteps, n_features, provided_input = None):\n",
    "\n",
    "    #Input\n",
    "    if provided_input == None:\n",
    "        x = Input((n_timesteps, n_features))\n",
    "    else:\n",
    "        x = provided_input\n",
    "        \n",
    "    #Dense\n",
    "    dense_1 = Lambda(lambda x: K.tf.unstack(x, axis=2))(x)\n",
    "    dense_2 = [Dense(20)(x) for x in dense_1]\n",
    "    dense_3 = Lambda(lambda x: K.stack(x, axis=2))(dense_2)\n",
    "    dense_4 = Dropout(0.1)(dense_3)\n",
    "    dense_5 = Flatten()(dense_4)\n",
    "    dense_6 = Dense(250, activation = 'relu')(dense_5)\n",
    "    dense_7 = Dense(20, activation = 'relu')(dense_6)\n",
    "    dense_8 = Dense(n_outputs, activation='softmax')(dense_7)\n",
    "    \n",
    "    return x, dense_8, dense_6\n",
    "\n",
    "\n",
    "def lstm_model_generator(n_timesteps, n_features, provided_input = None):\n",
    "    \n",
    "    #Input\n",
    "    if provided_input == None:\n",
    "        x = Input((n_timesteps, n_features))\n",
    "    else:\n",
    "        x = provided_input\n",
    "\n",
    "    #LSTM\n",
    "    lstm_1 = LSTM(100, input_shape=(n_timesteps,n_features))(x)\n",
    "    lstm_2 = Dropout(0.5)(lstm_1)\n",
    "    lstm_3 = Dense(100, activation='relu')(lstm_2)\n",
    "    lstm_4 = Dense(n_outputs, activation='softmax', name='lstm_out')(lstm_3)\n",
    "    \n",
    "    return x, lstm_4, lstm_3\n",
    "\n",
    "\n",
    "def hibrid_ens_generator(n_timesteps, n_features):\n",
    "\n",
    "    dense_in, dense_out, dense_int = dense_model_generator(n_timesteps, n_features)\n",
    "    lstm_in, lstm_out, lstm_int  = lstm_model_generator(n_timesteps, n_features, provided_input=dense_in)\n",
    "\n",
    "    ens_1 = Concatenate(axis=1)([lstm_int,dense_int])\n",
    "    ens_2 = Dense(n_outputs, activation='softmax')(ens_1)\n",
    "    \n",
    "    return dense_in, ens_2, ens_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5881 samples, validate on 1471 samples\n",
      "Epoch 1/100\n",
      "5881/5881 [==============================] - 5s 777us/step - loss: 1.7172 - acc: 0.3226 - val_loss: 1.5621 - val_acc: 0.4439\n",
      "Epoch 2/100\n",
      "5881/5881 [==============================] - 4s 640us/step - loss: 1.3972 - acc: 0.4023 - val_loss: 1.3558 - val_acc: 0.4521\n",
      "Epoch 3/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 1.2073 - acc: 0.4727 - val_loss: 1.1386 - val_acc: 0.4691\n",
      "Epoch 4/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 1.0849 - acc: 0.5195 - val_loss: 1.0738 - val_acc: 0.5452\n",
      "Epoch 5/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 1.0175 - acc: 0.5552 - val_loss: 1.0296 - val_acc: 0.5948\n",
      "Epoch 6/100\n",
      "5881/5881 [==============================] - 4s 639us/step - loss: 0.9429 - acc: 0.5967 - val_loss: 0.9388 - val_acc: 0.6023\n",
      "Epoch 7/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.8041 - acc: 0.6524 - val_loss: 0.8692 - val_acc: 0.6649\n",
      "Epoch 8/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.7987 - acc: 0.6465 - val_loss: 0.9001 - val_acc: 0.6513\n",
      "Epoch 9/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.7598 - acc: 0.6688 - val_loss: 0.7805 - val_acc: 0.7029\n",
      "Epoch 10/100\n",
      "5881/5881 [==============================] - 4s 640us/step - loss: 0.7032 - acc: 0.6807 - val_loss: 0.7907 - val_acc: 0.7267\n",
      "Epoch 11/100\n",
      "5881/5881 [==============================] - 4s 645us/step - loss: 0.6870 - acc: 0.6943 - val_loss: 0.7962 - val_acc: 0.6880\n",
      "Epoch 12/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.6498 - acc: 0.7135 - val_loss: 0.7664 - val_acc: 0.7274\n",
      "Epoch 13/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.6019 - acc: 0.7404 - val_loss: 0.7215 - val_acc: 0.7390\n",
      "Epoch 14/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.5588 - acc: 0.7574 - val_loss: 0.7087 - val_acc: 0.7444\n",
      "Epoch 15/100\n",
      "5881/5881 [==============================] - 4s 640us/step - loss: 0.5137 - acc: 0.7800 - val_loss: 0.7354 - val_acc: 0.7505\n",
      "Epoch 16/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.5444 - acc: 0.7643 - val_loss: 0.6934 - val_acc: 0.7498\n",
      "Epoch 17/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.5182 - acc: 0.7771 - val_loss: 0.6780 - val_acc: 0.7689\n",
      "Epoch 18/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.4768 - acc: 0.7999 - val_loss: 0.7043 - val_acc: 0.7791\n",
      "Epoch 19/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.4508 - acc: 0.8145 - val_loss: 0.7328 - val_acc: 0.7403\n",
      "Epoch 20/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.4512 - acc: 0.8101 - val_loss: 0.6616 - val_acc: 0.7899\n",
      "Epoch 21/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.4094 - acc: 0.8308 - val_loss: 0.7397 - val_acc: 0.7791\n",
      "Epoch 22/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.4547 - acc: 0.8329 - val_loss: 0.6531 - val_acc: 0.8035\n",
      "Epoch 23/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.3865 - acc: 0.8544 - val_loss: 0.6097 - val_acc: 0.8212\n",
      "Epoch 24/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.3792 - acc: 0.8539 - val_loss: 0.5915 - val_acc: 0.8334\n",
      "Epoch 25/100\n",
      "5881/5881 [==============================] - 4s 640us/step - loss: 0.3509 - acc: 0.8592 - val_loss: 0.5632 - val_acc: 0.8559\n",
      "Epoch 26/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.3354 - acc: 0.8784 - val_loss: 0.5743 - val_acc: 0.8647\n",
      "Epoch 27/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.3077 - acc: 0.8971 - val_loss: 0.6760 - val_acc: 0.8443\n",
      "Epoch 28/100\n",
      "5881/5881 [==============================] - 4s 645us/step - loss: 0.2862 - acc: 0.9014 - val_loss: 0.5248 - val_acc: 0.8722\n",
      "Epoch 29/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.2901 - acc: 0.9135 - val_loss: 0.5564 - val_acc: 0.8681\n",
      "Epoch 30/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.3840 - acc: 0.8946 - val_loss: 0.5784 - val_acc: 0.8321\n",
      "Epoch 31/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.3451 - acc: 0.8857 - val_loss: 0.6111 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 32/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.3024 - acc: 0.8993 - val_loss: 0.6030 - val_acc: 0.8484\n",
      "Epoch 33/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2837 - acc: 0.9017 - val_loss: 0.6055 - val_acc: 0.8511\n",
      "Epoch 34/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2680 - acc: 0.9085 - val_loss: 0.5831 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 35/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2564 - acc: 0.9087 - val_loss: 0.5868 - val_acc: 0.8498\n",
      "Epoch 36/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.2501 - acc: 0.9123 - val_loss: 0.5871 - val_acc: 0.8518\n",
      "Epoch 37/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2461 - acc: 0.9153 - val_loss: 0.5779 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 38/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2404 - acc: 0.9175 - val_loss: 0.5639 - val_acc: 0.8620\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a79d5d240>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'LSTM'\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(model_name + '_' + str(time())))\n",
    "callbacks_lstm = callbacks + [tensorboard]\n",
    "\n",
    "lstm_input, lstm_output, _  = lstm_model_generator(n_timesteps, n_features)\n",
    "\n",
    "model = Model(lstm_input, lstm_output, name= model_name )\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, \n",
    "          train_y, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=validation_split_on_training,\n",
    "          verbose=True,\n",
    "         callbacks = callbacks_lstm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some comments about this architecture:\n",
    "\n",
    "- Note that the unstack dim is the feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128, 9)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               [(None, 128), (None, 0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           2580        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           2580        lambda_1[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           2580        lambda_1[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           2580        lambda_1[0][3]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 20)           2580        lambda_1[0][4]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 20)           2580        lambda_1[0][5]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 20)           2580        lambda_1[0][6]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 20)           2580        lambda_1[0][7]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 20)           2580        lambda_1[0][8]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 20, 9)        0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 9)        0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 180)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 250)          45250       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 20)           5020        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 6)            126         dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 73,616\n",
      "Trainable params: 73,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 5881 samples, validate on 1471 samples\n",
      "Epoch 1/100\n",
      "5881/5881 [==============================] - 1s 122us/step - loss: 1.4824 - acc: 0.4066 - val_loss: 1.2001 - val_acc: 0.5982\n",
      "Epoch 2/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 1.0224 - acc: 0.6410 - val_loss: 0.9493 - val_acc: 0.7301\n",
      "Epoch 3/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.7561 - acc: 0.7358 - val_loss: 0.8215 - val_acc: 0.7940\n",
      "Epoch 4/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.5797 - acc: 0.8153 - val_loss: 0.7016 - val_acc: 0.8328\n",
      "Epoch 5/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.4491 - acc: 0.8560 - val_loss: 0.6055 - val_acc: 0.8477\n",
      "Epoch 6/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.3522 - acc: 0.8835 - val_loss: 0.5535 - val_acc: 0.8491\n",
      "Epoch 7/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.2804 - acc: 0.9094 - val_loss: 0.5151 - val_acc: 0.8566\n",
      "Epoch 8/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.2390 - acc: 0.9196 - val_loss: 0.4866 - val_acc: 0.8681\n",
      "Epoch 9/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.2054 - acc: 0.9277 - val_loss: 0.4590 - val_acc: 0.8695\n",
      "Epoch 10/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1840 - acc: 0.9337 - val_loss: 0.4359 - val_acc: 0.8756\n",
      "Epoch 11/100\n",
      "5881/5881 [==============================] - 0s 36us/step - loss: 0.1631 - acc: 0.9437 - val_loss: 0.4413 - val_acc: 0.8885\n",
      "Epoch 12/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.1542 - acc: 0.9456 - val_loss: 0.4283 - val_acc: 0.8776\n",
      "Epoch 13/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1409 - acc: 0.9471 - val_loss: 0.4164 - val_acc: 0.8824\n",
      "Epoch 14/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.1321 - acc: 0.9507 - val_loss: 0.3992 - val_acc: 0.8899\n",
      "Epoch 15/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.1261 - acc: 0.9527 - val_loss: 0.3960 - val_acc: 0.8878\n",
      "Epoch 16/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1164 - acc: 0.9561 - val_loss: 0.4016 - val_acc: 0.8912\n",
      "Epoch 17/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1145 - acc: 0.9544 - val_loss: 0.4001 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 18/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.1074 - acc: 0.9583 - val_loss: 0.3885 - val_acc: 0.8953\n",
      "Epoch 19/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1051 - acc: 0.9594 - val_loss: 0.3883 - val_acc: 0.8946\n",
      "Epoch 20/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1029 - acc: 0.9582 - val_loss: 0.3818 - val_acc: 0.8980\n",
      "Epoch 21/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0997 - acc: 0.9592 - val_loss: 0.3790 - val_acc: 0.8967\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 22/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1004 - acc: 0.9587 - val_loss: 0.3839 - val_acc: 0.8967\n",
      "Epoch 23/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0985 - acc: 0.9599 - val_loss: 0.3776 - val_acc: 0.8960\n",
      "Epoch 24/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0965 - acc: 0.9616 - val_loss: 0.3795 - val_acc: 0.8960\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0959 - acc: 0.9602 - val_loss: 0.3773 - val_acc: 0.8953\n",
      "Epoch 26/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0955 - acc: 0.9626 - val_loss: 0.3772 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 27/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0954 - acc: 0.9600 - val_loss: 0.3752 - val_acc: 0.8960\n",
      "Epoch 28/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0946 - acc: 0.9600 - val_loss: 0.3762 - val_acc: 0.8967\n",
      "Epoch 29/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0938 - acc: 0.9616 - val_loss: 0.3761 - val_acc: 0.8953\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 30/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0935 - acc: 0.9606 - val_loss: 0.3770 - val_acc: 0.8953\n",
      "Epoch 31/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0920 - acc: 0.9623 - val_loss: 0.3765 - val_acc: 0.8946\n",
      "Epoch 32/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0943 - acc: 0.9638 - val_loss: 0.3778 - val_acc: 0.8953\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 33/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0929 - acc: 0.9617 - val_loss: 0.3787 - val_acc: 0.8960\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00033: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a6070bf60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'dense'\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(model_name + '_' + str(time())))\n",
    "callbacks_dense = callbacks + [tensorboard]\n",
    "\n",
    "#generate model\n",
    "dense_input, dense_output , _ = dense_model_generator(n_timesteps, n_features)\n",
    "model = Model(dense_input, dense_output, name = model_name)\n",
    "\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(train_x, \n",
    "          train_y, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=validation_split_on_training,\n",
    "          verbose=True,\n",
    "         callbacks = callbacks_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 128, 9)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               [(None, 128), (None, 0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 20)           2580        lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 20)           2580        lambda_3[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 20)           2580        lambda_3[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 20)           2580        lambda_3[0][3]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 20)           2580        lambda_3[0][4]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 20)           2580        lambda_3[0][5]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 20)           2580        lambda_3[0][6]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 20)           2580        lambda_3[0][7]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 20)           2580        lambda_3[0][8]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 20, 9)        0           dense_14[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "                                                                 dense_16[0][0]                   \n",
      "                                                                 dense_17[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "                                                                 dense_21[0][0]                   \n",
      "                                                                 dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100)          44000       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 9)        0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100)          0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 180)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 100)          10100       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 250)          45250       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 350)          0           dense_26[0][0]                   \n",
      "                                                                 dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 6)            2106        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 124,676\n",
      "Trainable params: 124,676\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 5881 samples, validate on 1471 samples\n",
      "Epoch 1/100\n",
      "5881/5881 [==============================] - 5s 851us/step - loss: 1.4611 - acc: 0.4030 - val_loss: 1.1512 - val_acc: 0.6390\n",
      "Epoch 2/100\n",
      "5881/5881 [==============================] - 4s 646us/step - loss: 0.9612 - acc: 0.6478 - val_loss: 0.9848 - val_acc: 0.7124\n",
      "Epoch 3/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.7437 - acc: 0.7381 - val_loss: 0.8530 - val_acc: 0.7899\n",
      "Epoch 4/100\n",
      "5881/5881 [==============================] - 4s 645us/step - loss: 0.5804 - acc: 0.8145 - val_loss: 0.7434 - val_acc: 0.8232\n",
      "Epoch 5/100\n",
      "5881/5881 [==============================] - 4s 651us/step - loss: 0.4586 - acc: 0.8602 - val_loss: 0.6406 - val_acc: 0.8396\n",
      "Epoch 6/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.3574 - acc: 0.8818 - val_loss: 0.5671 - val_acc: 0.8532\n",
      "Epoch 7/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.2943 - acc: 0.9017 - val_loss: 0.5249 - val_acc: 0.8579\n",
      "Epoch 8/100\n",
      "5881/5881 [==============================] - 4s 646us/step - loss: 0.2465 - acc: 0.9163 - val_loss: 0.5015 - val_acc: 0.8790\n",
      "Epoch 9/100\n",
      "5881/5881 [==============================] - 4s 651us/step - loss: 0.2105 - acc: 0.9313 - val_loss: 0.4824 - val_acc: 0.8804\n",
      "Epoch 10/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.1854 - acc: 0.9388 - val_loss: 0.4697 - val_acc: 0.8831\n",
      "Epoch 11/100\n",
      "5881/5881 [==============================] - 4s 647us/step - loss: 0.1674 - acc: 0.9425 - val_loss: 0.4650 - val_acc: 0.8939\n",
      "Epoch 12/100\n",
      "5881/5881 [==============================] - 4s 652us/step - loss: 0.1619 - acc: 0.9454 - val_loss: 0.4525 - val_acc: 0.8960\n",
      "Epoch 13/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.1453 - acc: 0.9492 - val_loss: 0.4537 - val_acc: 0.8926\n",
      "Epoch 14/100\n",
      "5881/5881 [==============================] - 4s 649us/step - loss: 0.1316 - acc: 0.9537 - val_loss: 0.4433 - val_acc: 0.8994\n",
      "Epoch 15/100\n",
      "5881/5881 [==============================] - 4s 648us/step - loss: 0.1223 - acc: 0.9568 - val_loss: 0.4502 - val_acc: 0.8987\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 16/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.1159 - acc: 0.9585 - val_loss: 0.4592 - val_acc: 0.9001\n",
      "Epoch 17/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.1118 - acc: 0.9577 - val_loss: 0.4616 - val_acc: 0.9001\n",
      "Epoch 18/100\n",
      "5881/5881 [==============================] - 4s 648us/step - loss: 0.1097 - acc: 0.9594 - val_loss: 0.4645 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 19/100\n",
      "5881/5881 [==============================] - 4s 652us/step - loss: 0.1057 - acc: 0.9609 - val_loss: 0.4655 - val_acc: 0.9014\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5881/5881 [==============================] - 4s 649us/step - loss: 0.1047 - acc: 0.9587 - val_loss: 0.4648 - val_acc: 0.9007\n",
      "Epoch 21/100\n",
      "5881/5881 [==============================] - 4s 649us/step - loss: 0.1036 - acc: 0.9606 - val_loss: 0.4663 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 22/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.1019 - acc: 0.9612 - val_loss: 0.4661 - val_acc: 0.9014\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f495c6a25f8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ens'\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(model_name + '_' + str(time())))\n",
    "callbacks_ens = callbacks + [tensorboard]\n",
    "\n",
    "ens_input, ens_output, _  = hibrid_ens_generator(n_timesteps, n_features)\n",
    "\n",
    "\n",
    "model = Model(ens_input, ens_output, name= model_name)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_x, \n",
    "          train_y, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=validation_split_on_training,\n",
    "          verbose=True,\n",
    "         callbacks = callbacks_ens)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
