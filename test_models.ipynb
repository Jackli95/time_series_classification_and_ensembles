{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on \n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/\n",
    "\n",
    "https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_tensorboard import TrainValTensorBoard\n",
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = dstack(loaded)\n",
    "\treturn loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix)\n",
    "    #trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix)\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_dataset('../data/HAR/UCI_HAR_Dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 128, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = x_train.shape[1], x_train.shape[2], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Lambda, Input, Dropout, Flatten, LSTM, Concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "auc_roc = as_keras_metric(tf.metrics.auc)\n",
    "recall = as_keras_metric(tf.metrics.recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cb = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, min_delta = 0.01, patience = 3, verbose = 1)\n",
    "es_cb = EarlyStopping(monitor = 'val_loss', min_delta=0.01, patience = 10, verbose = 1, restore_best_weights = True)\n",
    "callbacks = [lr_cb, es_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.adam(lr=0.01)\n",
    "validation_split_on_training = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_model_generator(n_timesteps, n_features, provided_input = None):\n",
    "\n",
    "    #Input\n",
    "    if provided_input == None:\n",
    "        x = Input((n_timesteps, n_features))\n",
    "    else:\n",
    "        x = provided_input\n",
    "        \n",
    "    #Dense\n",
    "    dense_1 = Lambda(lambda x: K.tf.unstack(x, axis=2))(x)\n",
    "    dense_2 = [Dense(20)(x) for x in dense_1]\n",
    "    dense_3 = Lambda(lambda x: K.stack(x, axis=2))(dense_2)\n",
    "    dense_4 = Dropout(0.1)(dense_3)\n",
    "    dense_5 = Flatten()(dense_4)\n",
    "    dense_6 = Dense(250, activation = 'relu')(dense_5)\n",
    "    dense_7 = Dense(20, activation = 'relu')(dense_6)\n",
    "    dense_8 = Dense(n_outputs, activation='softmax', name = 'dense_out')(dense_7)\n",
    "    \n",
    "    return x, dense_8, dense_6\n",
    "\n",
    "\n",
    "def lstm_model_generator(n_timesteps, n_features, provided_input = None):\n",
    "    \n",
    "    #Input\n",
    "    if provided_input == None:\n",
    "        x = Input((n_timesteps, n_features))\n",
    "    else:\n",
    "        x = provided_input\n",
    "\n",
    "    #LSTM\n",
    "    lstm_1 = LSTM(100, input_shape=(n_timesteps,n_features))(x)\n",
    "    lstm_2 = Dropout(0.5)(lstm_1)\n",
    "    lstm_3 = Dense(100, activation='relu')(lstm_2)\n",
    "    lstm_4 = Dense(n_outputs, activation='softmax', name='lstm_out')(lstm_3)\n",
    "    \n",
    "    return x, lstm_4, lstm_3\n",
    "\n",
    "\n",
    "def hibrid_ens_generator(n_timesteps, n_features):\n",
    "\n",
    "    dense_in, dense_out, dense_int = dense_model_generator(n_timesteps, n_features)\n",
    "    lstm_in, lstm_out, lstm_int  = lstm_model_generator(n_timesteps, n_features, provided_input=dense_in)\n",
    "\n",
    "    ens_1 = Concatenate(axis=1)([lstm_int,dense_int])\n",
    "    ens_2 = Dense(n_outputs, activation='softmax')(ens_1)\n",
    "    \n",
    "    return dense_in, ens_2, ens_1\n",
    "\n",
    "\n",
    "def hibrid_ens_2_generator(n_timesteps, n_features):\n",
    "\n",
    "    dense_in, dense_out, dense_int = dense_model_generator(n_timesteps, n_features)\n",
    "    lstm_in, lstm_out, lstm_int  = lstm_model_generator(n_timesteps, n_features, provided_input=dense_in)\n",
    "\n",
    "    ens_1 = Concatenate(axis=1)([lstm_int,dense_int])\n",
    "    ens_2 = Dense(n_outputs, activation='softmax', name = 'ens_out')(ens_1)\n",
    "    \n",
    "    return dense_in, [dense_out, lstm_out, ens_2], ens_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 128, 9)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              [(None, 128), (None, 0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_90 (Dense)                (None, 20)           2580        lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_91 (Dense)                (None, 20)           2580        lambda_15[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 20)           2580        lambda_15[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 20)           2580        lambda_15[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_94 (Dense)                (None, 20)           2580        lambda_15[0][4]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_95 (Dense)                (None, 20)           2580        lambda_15[0][5]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_96 (Dense)                (None, 20)           2580        lambda_15[0][6]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 20)           2580        lambda_15[0][7]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 20)           2580        lambda_15[0][8]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 20, 9)        0           dense_90[0][0]                   \n",
      "                                                                 dense_91[0][0]                   \n",
      "                                                                 dense_92[0][0]                   \n",
      "                                                                 dense_93[0][0]                   \n",
      "                                                                 dense_94[0][0]                   \n",
      "                                                                 dense_95[0][0]                   \n",
      "                                                                 dense_96[0][0]                   \n",
      "                                                                 dense_97[0][0]                   \n",
      "                                                                 dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 20, 9)        0           lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 100)          44000       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 180)          0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 100)          0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 250)          45250       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 100)          10100       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 20)           5020        dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 350)          0           dense_101[0][0]                  \n",
      "                                                                 dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_out (Dense)               (None, 6)            126         dense_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_out (Dense)                (None, 6)            606         dense_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ens_out (Dense)                 (None, 6)            2106        concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 130,428\n",
      "Trainable params: 130,428\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 5881 samples, validate on 1471 samples\n",
      "Epoch 1/100\n",
      "5881/5881 [==============================] - 8s 1ms/step - loss: 1.4847 - dense_out_loss: 1.5304 - lstm_out_loss: 1.7268 - ens_out_loss: 1.3887 - dense_out_acc: 0.4526 - dense_out_auc: 0.5899 - lstm_out_acc: 0.2455 - lstm_out_auc: 0.5606 - ens_out_acc: 0.4877 - ens_out_auc: 0.6288 - val_loss: 1.2342 - val_dense_out_loss: 1.2423 - val_lstm_out_loss: 1.6063 - val_ens_out_loss: 1.1075 - val_dense_out_acc: 0.5812 - val_dense_out_auc: 0.7612 - val_lstm_out_acc: 0.3569 - val_lstm_out_auc: 0.6681 - val_ens_out_acc: 0.6465 - val_ens_out_auc: 0.8167\n",
      "Epoch 2/100\n",
      "5881/5881 [==============================] - 4s 671us/step - loss: 1.0376 - dense_out_loss: 1.0447 - lstm_out_loss: 1.4218 - ens_out_loss: 0.9072 - dense_out_acc: 0.6149 - dense_out_auc: 0.8076 - lstm_out_acc: 0.4023 - lstm_out_auc: 0.6987 - ens_out_acc: 0.6769 - ens_out_auc: 0.8548 - val_loss: 1.0023 - val_dense_out_loss: 0.9884 - val_lstm_out_loss: 1.1866 - val_ens_out_loss: 0.9456 - val_dense_out_acc: 0.6968 - val_dense_out_auc: 0.8392 - val_lstm_out_acc: 0.4881 - val_lstm_out_auc: 0.7290 - val_ens_out_acc: 0.7573 - val_ens_out_auc: 0.8799\n",
      "Epoch 3/100\n",
      "5881/5881 [==============================] - 4s 691us/step - loss: 0.8149 - dense_out_loss: 0.7949 - lstm_out_loss: 1.1935 - ens_out_loss: 0.6953 - dense_out_acc: 0.7155 - dense_out_auc: 0.8603 - lstm_out_acc: 0.4912 - lstm_out_auc: 0.7520 - ens_out_acc: 0.7691 - ens_out_auc: 0.8957 - val_loss: 0.8761 - val_dense_out_loss: 0.8134 - val_lstm_out_loss: 1.1852 - val_ens_out_loss: 0.7940 - val_dense_out_acc: 0.7661 - val_dense_out_auc: 0.8792 - val_lstm_out_acc: 0.4453 - val_lstm_out_auc: 0.7704 - val_ens_out_acc: 0.8103 - val_ens_out_auc: 0.9101\n",
      "Epoch 4/100\n",
      "5881/5881 [==============================] - 4s 700us/step - loss: 0.7190 - dense_out_loss: 0.6142 - lstm_out_loss: 1.3521 - ens_out_loss: 0.5429 - dense_out_acc: 0.7951 - dense_out_auc: 0.8936 - lstm_out_acc: 0.4659 - lstm_out_auc: 0.7815 - ens_out_acc: 0.8305 - ens_out_auc: 0.9207 - val_loss: 0.8799 - val_dense_out_loss: 0.6941 - val_lstm_out_loss: 1.6049 - val_ens_out_loss: 0.7001 - val_dense_out_acc: 0.8199 - val_dense_out_auc: 0.9061 - val_lstm_out_acc: 0.3576 - val_lstm_out_auc: 0.7805 - val_ens_out_acc: 0.8171 - val_ens_out_auc: 0.9296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "5881/5881 [==============================] - 4s 696us/step - loss: 0.6186 - dense_out_loss: 0.4782 - lstm_out_loss: 1.2810 - ens_out_loss: 0.4447 - dense_out_acc: 0.8407 - dense_out_auc: 0.9157 - lstm_out_acc: 0.4360 - lstm_out_auc: 0.7802 - ens_out_acc: 0.8638 - ens_out_auc: 0.9360 - val_loss: 0.7219 - val_dense_out_loss: 0.5956 - val_lstm_out_loss: 1.2055 - val_ens_out_loss: 0.6028 - val_dense_out_acc: 0.8260 - val_dense_out_auc: 0.9244 - val_lstm_out_acc: 0.4575 - val_lstm_out_auc: 0.7869 - val_ens_out_acc: 0.8382 - val_ens_out_auc: 0.9420\n",
      "Epoch 6/100\n",
      "5881/5881 [==============================] - 4s 689us/step - loss: 0.5085 - dense_out_loss: 0.3722 - lstm_out_loss: 1.1527 - ens_out_loss: 0.3391 - dense_out_acc: 0.8675 - dense_out_auc: 0.9315 - lstm_out_acc: 0.5169 - lstm_out_auc: 0.7940 - ens_out_acc: 0.8932 - ens_out_auc: 0.9470 - val_loss: 0.6661 - val_dense_out_loss: 0.5429 - val_lstm_out_loss: 1.1189 - val_ens_out_loss: 0.5563 - val_dense_out_acc: 0.8382 - val_dense_out_auc: 0.9377 - val_lstm_out_acc: 0.5241 - val_lstm_out_auc: 0.8013 - val_ens_out_acc: 0.8525 - val_ens_out_auc: 0.9516\n",
      "Epoch 7/100\n",
      "5881/5881 [==============================] - 4s 699us/step - loss: 0.4390 - dense_out_loss: 0.2993 - lstm_out_loss: 1.0563 - ens_out_loss: 0.2799 - dense_out_acc: 0.8915 - dense_out_auc: 0.9428 - lstm_out_acc: 0.5557 - lstm_out_auc: 0.8081 - ens_out_acc: 0.9061 - ens_out_auc: 0.9553 - val_loss: 0.6206 - val_dense_out_loss: 0.4965 - val_lstm_out_loss: 1.0604 - val_ens_out_loss: 0.5153 - val_dense_out_acc: 0.8613 - val_dense_out_auc: 0.9474 - val_lstm_out_acc: 0.6023 - val_lstm_out_auc: 0.8148 - val_ens_out_acc: 0.8593 - val_ens_out_auc: 0.9586\n",
      "Epoch 8/100\n",
      "5881/5881 [==============================] - 4s 699us/step - loss: 0.3888 - dense_out_loss: 0.2452 - lstm_out_loss: 0.9878 - ens_out_loss: 0.2371 - dense_out_acc: 0.9117 - dense_out_auc: 0.9513 - lstm_out_acc: 0.5865 - lstm_out_auc: 0.8210 - ens_out_acc: 0.9203 - ens_out_auc: 0.9613 - val_loss: 0.5849 - val_dense_out_loss: 0.4648 - val_lstm_out_loss: 1.0087 - val_ens_out_loss: 0.4837 - val_dense_out_acc: 0.8640 - val_dense_out_auc: 0.9549 - val_lstm_out_acc: 0.6492 - val_lstm_out_auc: 0.8268 - val_ens_out_acc: 0.8640 - val_ens_out_auc: 0.9639\n",
      "Epoch 9/100\n",
      "5881/5881 [==============================] - 4s 700us/step - loss: 0.3547 - dense_out_loss: 0.2121 - lstm_out_loss: 0.9317 - ens_out_loss: 0.2099 - dense_out_acc: 0.9228 - dense_out_auc: 0.9579 - lstm_out_acc: 0.6101 - lstm_out_auc: 0.8324 - ens_out_acc: 0.9265 - ens_out_auc: 0.9660 - val_loss: 0.5609 - val_dense_out_loss: 0.4487 - val_lstm_out_loss: 0.9634 - val_ens_out_loss: 0.4641 - val_dense_out_acc: 0.8742 - val_dense_out_auc: 0.9607 - val_lstm_out_acc: 0.6288 - val_lstm_out_auc: 0.8377 - val_ens_out_acc: 0.8681 - val_ens_out_auc: 0.9680\n",
      "Epoch 10/100\n",
      "5881/5881 [==============================] - 4s 695us/step - loss: 0.3234 - dense_out_loss: 0.1874 - lstm_out_loss: 0.8609 - ens_out_loss: 0.1895 - dense_out_acc: 0.9335 - dense_out_auc: 0.9630 - lstm_out_acc: 0.6405 - lstm_out_auc: 0.8430 - ens_out_acc: 0.9383 - ens_out_auc: 0.9696 - val_loss: 0.5580 - val_dense_out_loss: 0.4438 - val_lstm_out_loss: 0.9875 - val_ens_out_loss: 0.4528 - val_dense_out_acc: 0.8661 - val_dense_out_auc: 0.9653 - val_lstm_out_acc: 0.6159 - val_lstm_out_auc: 0.8477 - val_ens_out_acc: 0.8729 - val_ens_out_auc: 0.9712\n",
      "Epoch 11/100\n",
      "5881/5881 [==============================] - 4s 699us/step - loss: 0.3021 - dense_out_loss: 0.1682 - lstm_out_loss: 0.8274 - ens_out_loss: 0.1716 - dense_out_acc: 0.9405 - dense_out_auc: 0.9671 - lstm_out_acc: 0.6552 - lstm_out_auc: 0.8519 - ens_out_acc: 0.9415 - ens_out_auc: 0.9726 - val_loss: 0.5281 - val_dense_out_loss: 0.4369 - val_lstm_out_loss: 0.8783 - val_ens_out_loss: 0.4418 - val_dense_out_acc: 0.8715 - val_dense_out_auc: 0.9689 - val_lstm_out_acc: 0.7131 - val_lstm_out_auc: 0.8568 - val_ens_out_acc: 0.8756 - val_ens_out_auc: 0.9739\n",
      "Epoch 12/100\n",
      "5881/5881 [==============================] - 4s 698us/step - loss: 0.2733 - dense_out_loss: 0.1540 - lstm_out_loss: 0.7415 - ens_out_loss: 0.1570 - dense_out_acc: 0.9437 - dense_out_auc: 0.9704 - lstm_out_acc: 0.6970 - lstm_out_auc: 0.8619 - ens_out_acc: 0.9424 - ens_out_auc: 0.9750 - val_loss: 0.5113 - val_dense_out_loss: 0.4254 - val_lstm_out_loss: 0.8269 - val_ens_out_loss: 0.4347 - val_dense_out_acc: 0.8783 - val_dense_out_auc: 0.9718 - val_lstm_out_acc: 0.7288 - val_lstm_out_auc: 0.8663 - val_ens_out_acc: 0.8844 - val_ens_out_auc: 0.9761\n",
      "Epoch 13/100\n",
      "5881/5881 [==============================] - 4s 696us/step - loss: 0.2438 - dense_out_loss: 0.1422 - lstm_out_loss: 0.6489 - ens_out_loss: 0.1426 - dense_out_acc: 0.9490 - dense_out_auc: 0.9731 - lstm_out_acc: 0.7456 - lstm_out_auc: 0.8709 - ens_out_acc: 0.9473 - ens_out_auc: 0.9770 - val_loss: 0.5114 - val_dense_out_loss: 0.4251 - val_lstm_out_loss: 0.8384 - val_ens_out_loss: 0.4312 - val_dense_out_acc: 0.8844 - val_dense_out_auc: 0.9743 - val_lstm_out_acc: 0.7566 - val_lstm_out_auc: 0.8754 - val_ens_out_acc: 0.8858 - val_ens_out_auc: 0.9779\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/100\n",
      "5881/5881 [==============================] - 4s 700us/step - loss: 0.2253 - dense_out_loss: 0.1325 - lstm_out_loss: 0.5967 - ens_out_loss: 0.1325 - dense_out_acc: 0.9522 - dense_out_auc: 0.9753 - lstm_out_acc: 0.7686 - lstm_out_auc: 0.8795 - ens_out_acc: 0.9536 - ens_out_auc: 0.9787 - val_loss: 0.4991 - val_dense_out_loss: 0.4347 - val_lstm_out_loss: 0.7658 - val_ens_out_loss: 0.4317 - val_dense_out_acc: 0.8783 - val_dense_out_auc: 0.9763 - val_lstm_out_acc: 0.7940 - val_lstm_out_auc: 0.8837 - val_ens_out_acc: 0.8838 - val_ens_out_auc: 0.9795\n",
      "Epoch 15/100\n",
      "5881/5881 [==============================] - 4s 693us/step - loss: 0.2116 - dense_out_loss: 0.1267 - lstm_out_loss: 0.5495 - ens_out_loss: 0.1273 - dense_out_acc: 0.9485 - dense_out_auc: 0.9771 - lstm_out_acc: 0.7852 - lstm_out_auc: 0.8876 - ens_out_acc: 0.9481 - ens_out_auc: 0.9801 - val_loss: 0.4906 - val_dense_out_loss: 0.4171 - val_lstm_out_loss: 0.7627 - val_ens_out_loss: 0.4244 - val_dense_out_acc: 0.8892 - val_dense_out_auc: 0.9780 - val_lstm_out_acc: 0.7886 - val_lstm_out_auc: 0.8914 - val_ens_out_acc: 0.8919 - val_ens_out_auc: 0.9808\n",
      "Epoch 16/100\n",
      "5881/5881 [==============================] - 4s 703us/step - loss: 0.2057 - dense_out_loss: 0.1252 - lstm_out_loss: 0.5311 - ens_out_loss: 0.1241 - dense_out_acc: 0.9539 - dense_out_auc: 0.9787 - lstm_out_acc: 0.7988 - lstm_out_auc: 0.8949 - ens_out_acc: 0.9546 - ens_out_auc: 0.9814 - val_loss: 0.5040 - val_dense_out_loss: 0.4321 - val_lstm_out_loss: 0.7824 - val_ens_out_loss: 0.4352 - val_dense_out_acc: 0.8804 - val_dense_out_auc: 0.9794 - val_lstm_out_acc: 0.7770 - val_lstm_out_auc: 0.8980 - val_ens_out_acc: 0.8838 - val_ens_out_auc: 0.9819\n",
      "Epoch 17/100\n",
      "5881/5881 [==============================] - 4s 700us/step - loss: 0.2003 - dense_out_loss: 0.1205 - lstm_out_loss: 0.5204 - ens_out_loss: 0.1202 - dense_out_acc: 0.9548 - dense_out_auc: 0.9800 - lstm_out_acc: 0.7953 - lstm_out_auc: 0.9009 - ens_out_acc: 0.9536 - ens_out_auc: 0.9824 - val_loss: 0.5004 - val_dense_out_loss: 0.4247 - val_lstm_out_loss: 0.7707 - val_ens_out_loss: 0.4355 - val_dense_out_acc: 0.8885 - val_dense_out_auc: 0.9806 - val_lstm_out_acc: 0.7927 - val_lstm_out_auc: 0.9037 - val_ens_out_acc: 0.8919 - val_ens_out_auc: 0.9829\n",
      "Epoch 18/100\n",
      "5881/5881 [==============================] - 4s 701us/step - loss: 0.1864 - dense_out_loss: 0.1183 - lstm_out_loss: 0.4725 - ens_out_loss: 0.1138 - dense_out_acc: 0.9553 - dense_out_auc: 0.9812 - lstm_out_acc: 0.8155 - lstm_out_auc: 0.9064 - ens_out_acc: 0.9590 - ens_out_auc: 0.9833 - val_loss: 0.5053 - val_dense_out_loss: 0.4334 - val_lstm_out_loss: 0.7661 - val_ens_out_loss: 0.4424 - val_dense_out_acc: 0.8933 - val_dense_out_auc: 0.9817 - val_lstm_out_acc: 0.8124 - val_lstm_out_auc: 0.9091 - val_ens_out_acc: 0.8953 - val_ens_out_auc: 0.9837\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5881/5881 [==============================] - 4s 699us/step - loss: 0.1896 - dense_out_loss: 0.1158 - lstm_out_loss: 0.4868 - ens_out_loss: 0.1152 - dense_out_acc: 0.9565 - dense_out_auc: 0.9821 - lstm_out_acc: 0.8128 - lstm_out_auc: 0.9115 - ens_out_acc: 0.9573 - ens_out_auc: 0.9840 - val_loss: 0.5195 - val_dense_out_loss: 0.4331 - val_lstm_out_loss: 0.8346 - val_ens_out_loss: 0.4432 - val_dense_out_acc: 0.8858 - val_dense_out_auc: 0.9826 - val_lstm_out_acc: 0.7736 - val_lstm_out_auc: 0.9135 - val_ens_out_acc: 0.8933 - val_ens_out_auc: 0.9844\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 20/100\n",
      "5881/5881 [==============================] - 4s 700us/step - loss: 0.1857 - dense_out_loss: 0.1135 - lstm_out_loss: 0.4798 - ens_out_loss: 0.1117 - dense_out_acc: 0.9592 - dense_out_auc: 0.9830 - lstm_out_acc: 0.8111 - lstm_out_auc: 0.9154 - ens_out_acc: 0.9575 - ens_out_auc: 0.9847 - val_loss: 0.5093 - val_dense_out_loss: 0.4334 - val_lstm_out_loss: 0.7743 - val_ens_out_loss: 0.4462 - val_dense_out_acc: 0.8939 - val_dense_out_auc: 0.9834 - val_lstm_out_acc: 0.7906 - val_lstm_out_auc: 0.9174 - val_ens_out_acc: 0.8946 - val_ens_out_auc: 0.9850\n",
      "Epoch 21/100\n",
      "5881/5881 [==============================] - 4s 697us/step - loss: 0.1752 - dense_out_loss: 0.1097 - lstm_out_loss: 0.4428 - ens_out_loss: 0.1078 - dense_out_acc: 0.9585 - dense_out_auc: 0.9838 - lstm_out_acc: 0.8243 - lstm_out_auc: 0.9192 - ens_out_acc: 0.9583 - ens_out_auc: 0.9852 - val_loss: 0.5046 - val_dense_out_loss: 0.4343 - val_lstm_out_loss: 0.7479 - val_ens_out_loss: 0.4470 - val_dense_out_acc: 0.8933 - val_dense_out_auc: 0.9841 - val_lstm_out_acc: 0.8008 - val_lstm_out_auc: 0.9211 - val_ens_out_acc: 0.8960 - val_ens_out_auc: 0.9855\n",
      "Epoch 22/100\n",
      "5881/5881 [==============================] - 4s 697us/step - loss: 0.1711 - dense_out_loss: 0.1106 - lstm_out_loss: 0.4212 - ens_out_loss: 0.1079 - dense_out_acc: 0.9563 - dense_out_auc: 0.9844 - lstm_out_acc: 0.8407 - lstm_out_auc: 0.9229 - ens_out_acc: 0.9570 - ens_out_auc: 0.9858 - val_loss: 0.4989 - val_dense_out_loss: 0.4334 - val_lstm_out_loss: 0.7241 - val_ens_out_loss: 0.4456 - val_dense_out_acc: 0.8933 - val_dense_out_auc: 0.9848 - val_lstm_out_acc: 0.8035 - val_lstm_out_auc: 0.9247 - val_ens_out_acc: 0.8980 - val_ens_out_auc: 0.9860\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 23/100\n",
      "5881/5881 [==============================] - 4s 700us/step - loss: 0.1688 - dense_out_loss: 0.1081 - lstm_out_loss: 0.4157 - ens_out_loss: 0.1067 - dense_out_acc: 0.9590 - dense_out_auc: 0.9850 - lstm_out_acc: 0.8414 - lstm_out_auc: 0.9263 - ens_out_acc: 0.9568 - ens_out_auc: 0.9862 - val_loss: 0.5086 - val_dense_out_loss: 0.4357 - val_lstm_out_loss: 0.7589 - val_ens_out_loss: 0.4494 - val_dense_out_acc: 0.8933 - val_dense_out_auc: 0.9853 - val_lstm_out_acc: 0.8124 - val_lstm_out_auc: 0.9279 - val_ens_out_acc: 0.8967 - val_ens_out_auc: 0.9865\n",
      "Epoch 24/100\n",
      "5881/5881 [==============================] - 4s 699us/step - loss: 0.1648 - dense_out_loss: 0.1085 - lstm_out_loss: 0.4006 - ens_out_loss: 0.1051 - dense_out_acc: 0.9587 - dense_out_auc: 0.9856 - lstm_out_acc: 0.8495 - lstm_out_auc: 0.9293 - ens_out_acc: 0.9600 - ens_out_auc: 0.9866 - val_loss: 0.5064 - val_dense_out_loss: 0.4369 - val_lstm_out_loss: 0.7451 - val_ens_out_loss: 0.4500 - val_dense_out_acc: 0.8946 - val_dense_out_auc: 0.9858 - val_lstm_out_acc: 0.8090 - val_lstm_out_auc: 0.9307 - val_ens_out_acc: 0.8973 - val_ens_out_auc: 0.9868\n",
      "Epoch 25/100\n",
      "5881/5881 [==============================] - 4s 705us/step - loss: 0.1626 - dense_out_loss: 0.1059 - lstm_out_loss: 0.3952 - ens_out_loss: 0.1040 - dense_out_acc: 0.9590 - dense_out_auc: 0.9860 - lstm_out_acc: 0.8516 - lstm_out_auc: 0.9321 - ens_out_acc: 0.9587 - ens_out_auc: 0.9870 - val_loss: 0.5063 - val_dense_out_loss: 0.4361 - val_lstm_out_loss: 0.7431 - val_ens_out_loss: 0.4508 - val_dense_out_acc: 0.8946 - val_dense_out_auc: 0.9863 - val_lstm_out_acc: 0.8097 - val_lstm_out_auc: 0.9334 - val_ens_out_acc: 0.8973 - val_ens_out_auc: 0.9872\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 26/100\n",
      "5881/5881 [==============================] - 4s 701us/step - loss: 0.1628 - dense_out_loss: 0.1058 - lstm_out_loss: 0.3966 - ens_out_loss: 0.1039 - dense_out_acc: 0.9602 - dense_out_auc: 0.9865 - lstm_out_acc: 0.8480 - lstm_out_auc: 0.9346 - ens_out_acc: 0.9590 - ens_out_auc: 0.9873 - val_loss: 0.5102 - val_dense_out_loss: 0.4380 - val_lstm_out_loss: 0.7521 - val_ens_out_loss: 0.4537 - val_dense_out_acc: 0.8939 - val_dense_out_auc: 0.9867 - val_lstm_out_acc: 0.8226 - val_lstm_out_auc: 0.9357 - val_ens_out_acc: 0.8973 - val_ens_out_auc: 0.9875\n",
      "Epoch 27/100\n",
      "5881/5881 [==============================] - 4s 686us/step - loss: 0.1631 - dense_out_loss: 0.1065 - lstm_out_loss: 0.3951 - ens_out_loss: 0.1045 - dense_out_acc: 0.9583 - dense_out_auc: 0.9869 - lstm_out_acc: 0.8500 - lstm_out_auc: 0.9368 - ens_out_acc: 0.9573 - ens_out_auc: 0.9876 - val_loss: 0.5085 - val_dense_out_loss: 0.4378 - val_lstm_out_loss: 0.7441 - val_ens_out_loss: 0.4535 - val_dense_out_acc: 0.8939 - val_dense_out_auc: 0.9871 - val_lstm_out_acc: 0.8137 - val_lstm_out_auc: 0.9379 - val_ens_out_acc: 0.8980 - val_ens_out_auc: 0.9878\n",
      "Epoch 28/100\n",
      "5881/5881 [==============================] - 4s 700us/step - loss: 0.1609 - dense_out_loss: 0.1067 - lstm_out_loss: 0.3855 - ens_out_loss: 0.1042 - dense_out_acc: 0.9585 - dense_out_auc: 0.9872 - lstm_out_acc: 0.8577 - lstm_out_auc: 0.9389 - ens_out_acc: 0.9587 - ens_out_auc: 0.9879 - val_loss: 0.5082 - val_dense_out_loss: 0.4385 - val_lstm_out_loss: 0.7405 - val_ens_out_loss: 0.4541 - val_dense_out_acc: 0.8939 - val_dense_out_auc: 0.9874 - val_lstm_out_acc: 0.8192 - val_lstm_out_auc: 0.9399 - val_ens_out_acc: 0.8973 - val_ens_out_auc: 0.9880\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00028: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f167d835fd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ens_2'\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(model_name + '_' + str(time())))\n",
    "# custom callbacks\n",
    "lr_cb = ReduceLROnPlateau(monitor = 'val_ens_out_auc', mode = 'max',\n",
    "                          factor = 0.5, min_delta = 0.01, patience = 3, verbose = 1)\n",
    "es_cb = EarlyStopping(monitor = 'val_ens_out_auc',  mode = 'max',\n",
    "                      min_delta=0.005, patience = 10, verbose = 1, restore_best_weights = True)\n",
    "callbacks = [lr_cb, es_cb]\n",
    "callbacks_ens = callbacks\n",
    "\n",
    "ens_input, ens_output, _  = hibrid_ens_2_generator(n_timesteps, n_features)\n",
    "\n",
    "losses = ['categorical_crossentropy'] * len(ens_output)\n",
    "lossWeights = [0.2, 0.2, 0.6]\n",
    "\n",
    "model = Model(ens_input, ens_output, name= model_name)\n",
    "model.compile(loss=losses, loss_weights=lossWeights,\n",
    "              optimizer='adam', metrics=['accuracy', auc_roc])\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, \n",
    "          [y_train]*3, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=validation_split_on_training,\n",
    "          verbose=True,\n",
    "         callbacks = callbacks_ens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5881 samples, validate on 1471 samples\n",
      "Epoch 1/100\n",
      "5881/5881 [==============================] - 5s 777us/step - loss: 1.7172 - acc: 0.3226 - val_loss: 1.5621 - val_acc: 0.4439\n",
      "Epoch 2/100\n",
      "5881/5881 [==============================] - 4s 640us/step - loss: 1.3972 - acc: 0.4023 - val_loss: 1.3558 - val_acc: 0.4521\n",
      "Epoch 3/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 1.2073 - acc: 0.4727 - val_loss: 1.1386 - val_acc: 0.4691\n",
      "Epoch 4/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 1.0849 - acc: 0.5195 - val_loss: 1.0738 - val_acc: 0.5452\n",
      "Epoch 5/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 1.0175 - acc: 0.5552 - val_loss: 1.0296 - val_acc: 0.5948\n",
      "Epoch 6/100\n",
      "5881/5881 [==============================] - 4s 639us/step - loss: 0.9429 - acc: 0.5967 - val_loss: 0.9388 - val_acc: 0.6023\n",
      "Epoch 7/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.8041 - acc: 0.6524 - val_loss: 0.8692 - val_acc: 0.6649\n",
      "Epoch 8/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.7987 - acc: 0.6465 - val_loss: 0.9001 - val_acc: 0.6513\n",
      "Epoch 9/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.7598 - acc: 0.6688 - val_loss: 0.7805 - val_acc: 0.7029\n",
      "Epoch 10/100\n",
      "5881/5881 [==============================] - 4s 640us/step - loss: 0.7032 - acc: 0.6807 - val_loss: 0.7907 - val_acc: 0.7267\n",
      "Epoch 11/100\n",
      "5881/5881 [==============================] - 4s 645us/step - loss: 0.6870 - acc: 0.6943 - val_loss: 0.7962 - val_acc: 0.6880\n",
      "Epoch 12/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.6498 - acc: 0.7135 - val_loss: 0.7664 - val_acc: 0.7274\n",
      "Epoch 13/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.6019 - acc: 0.7404 - val_loss: 0.7215 - val_acc: 0.7390\n",
      "Epoch 14/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.5588 - acc: 0.7574 - val_loss: 0.7087 - val_acc: 0.7444\n",
      "Epoch 15/100\n",
      "5881/5881 [==============================] - 4s 640us/step - loss: 0.5137 - acc: 0.7800 - val_loss: 0.7354 - val_acc: 0.7505\n",
      "Epoch 16/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.5444 - acc: 0.7643 - val_loss: 0.6934 - val_acc: 0.7498\n",
      "Epoch 17/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.5182 - acc: 0.7771 - val_loss: 0.6780 - val_acc: 0.7689\n",
      "Epoch 18/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.4768 - acc: 0.7999 - val_loss: 0.7043 - val_acc: 0.7791\n",
      "Epoch 19/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.4508 - acc: 0.8145 - val_loss: 0.7328 - val_acc: 0.7403\n",
      "Epoch 20/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.4512 - acc: 0.8101 - val_loss: 0.6616 - val_acc: 0.7899\n",
      "Epoch 21/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.4094 - acc: 0.8308 - val_loss: 0.7397 - val_acc: 0.7791\n",
      "Epoch 22/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.4547 - acc: 0.8329 - val_loss: 0.6531 - val_acc: 0.8035\n",
      "Epoch 23/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.3865 - acc: 0.8544 - val_loss: 0.6097 - val_acc: 0.8212\n",
      "Epoch 24/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.3792 - acc: 0.8539 - val_loss: 0.5915 - val_acc: 0.8334\n",
      "Epoch 25/100\n",
      "5881/5881 [==============================] - 4s 640us/step - loss: 0.3509 - acc: 0.8592 - val_loss: 0.5632 - val_acc: 0.8559\n",
      "Epoch 26/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.3354 - acc: 0.8784 - val_loss: 0.5743 - val_acc: 0.8647\n",
      "Epoch 27/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.3077 - acc: 0.8971 - val_loss: 0.6760 - val_acc: 0.8443\n",
      "Epoch 28/100\n",
      "5881/5881 [==============================] - 4s 645us/step - loss: 0.2862 - acc: 0.9014 - val_loss: 0.5248 - val_acc: 0.8722\n",
      "Epoch 29/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.2901 - acc: 0.9135 - val_loss: 0.5564 - val_acc: 0.8681\n",
      "Epoch 30/100\n",
      "5881/5881 [==============================] - 4s 641us/step - loss: 0.3840 - acc: 0.8946 - val_loss: 0.5784 - val_acc: 0.8321\n",
      "Epoch 31/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.3451 - acc: 0.8857 - val_loss: 0.6111 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 32/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.3024 - acc: 0.8993 - val_loss: 0.6030 - val_acc: 0.8484\n",
      "Epoch 33/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2837 - acc: 0.9017 - val_loss: 0.6055 - val_acc: 0.8511\n",
      "Epoch 34/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2680 - acc: 0.9085 - val_loss: 0.5831 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 35/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2564 - acc: 0.9087 - val_loss: 0.5868 - val_acc: 0.8498\n",
      "Epoch 36/100\n",
      "5881/5881 [==============================] - 4s 643us/step - loss: 0.2501 - acc: 0.9123 - val_loss: 0.5871 - val_acc: 0.8518\n",
      "Epoch 37/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2461 - acc: 0.9153 - val_loss: 0.5779 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 38/100\n",
      "5881/5881 [==============================] - 4s 642us/step - loss: 0.2404 - acc: 0.9175 - val_loss: 0.5639 - val_acc: 0.8620\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a79d5d240>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'LSTM'\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(model_name + '_' + str(time())))\n",
    "callbacks_lstm = callbacks + [tensorboard]\n",
    "\n",
    "lstm_input, lstm_output, _  = lstm_model_generator(n_timesteps, n_features)\n",
    "\n",
    "model = Model(lstm_input, lstm_output, name= model_name )\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=validation_split_on_training,\n",
    "          verbose=True,\n",
    "         callbacks = callbacks_lstm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some comments about this architecture:\n",
    "\n",
    "- Note that the unstack dim is the feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128, 9)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               [(None, 128), (None, 0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           2580        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           2580        lambda_1[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           2580        lambda_1[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           2580        lambda_1[0][3]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 20)           2580        lambda_1[0][4]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 20)           2580        lambda_1[0][5]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 20)           2580        lambda_1[0][6]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 20)           2580        lambda_1[0][7]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 20)           2580        lambda_1[0][8]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 20, 9)        0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 9)        0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 180)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 250)          45250       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 20)           5020        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 6)            126         dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 73,616\n",
      "Trainable params: 73,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 5881 samples, validate on 1471 samples\n",
      "Epoch 1/100\n",
      "5881/5881 [==============================] - 1s 122us/step - loss: 1.4824 - acc: 0.4066 - val_loss: 1.2001 - val_acc: 0.5982\n",
      "Epoch 2/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 1.0224 - acc: 0.6410 - val_loss: 0.9493 - val_acc: 0.7301\n",
      "Epoch 3/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.7561 - acc: 0.7358 - val_loss: 0.8215 - val_acc: 0.7940\n",
      "Epoch 4/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.5797 - acc: 0.8153 - val_loss: 0.7016 - val_acc: 0.8328\n",
      "Epoch 5/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.4491 - acc: 0.8560 - val_loss: 0.6055 - val_acc: 0.8477\n",
      "Epoch 6/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.3522 - acc: 0.8835 - val_loss: 0.5535 - val_acc: 0.8491\n",
      "Epoch 7/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.2804 - acc: 0.9094 - val_loss: 0.5151 - val_acc: 0.8566\n",
      "Epoch 8/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.2390 - acc: 0.9196 - val_loss: 0.4866 - val_acc: 0.8681\n",
      "Epoch 9/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.2054 - acc: 0.9277 - val_loss: 0.4590 - val_acc: 0.8695\n",
      "Epoch 10/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1840 - acc: 0.9337 - val_loss: 0.4359 - val_acc: 0.8756\n",
      "Epoch 11/100\n",
      "5881/5881 [==============================] - 0s 36us/step - loss: 0.1631 - acc: 0.9437 - val_loss: 0.4413 - val_acc: 0.8885\n",
      "Epoch 12/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.1542 - acc: 0.9456 - val_loss: 0.4283 - val_acc: 0.8776\n",
      "Epoch 13/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1409 - acc: 0.9471 - val_loss: 0.4164 - val_acc: 0.8824\n",
      "Epoch 14/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.1321 - acc: 0.9507 - val_loss: 0.3992 - val_acc: 0.8899\n",
      "Epoch 15/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.1261 - acc: 0.9527 - val_loss: 0.3960 - val_acc: 0.8878\n",
      "Epoch 16/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1164 - acc: 0.9561 - val_loss: 0.4016 - val_acc: 0.8912\n",
      "Epoch 17/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1145 - acc: 0.9544 - val_loss: 0.4001 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 18/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.1074 - acc: 0.9583 - val_loss: 0.3885 - val_acc: 0.8953\n",
      "Epoch 19/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1051 - acc: 0.9594 - val_loss: 0.3883 - val_acc: 0.8946\n",
      "Epoch 20/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1029 - acc: 0.9582 - val_loss: 0.3818 - val_acc: 0.8980\n",
      "Epoch 21/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0997 - acc: 0.9592 - val_loss: 0.3790 - val_acc: 0.8967\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 22/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.1004 - acc: 0.9587 - val_loss: 0.3839 - val_acc: 0.8967\n",
      "Epoch 23/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0985 - acc: 0.9599 - val_loss: 0.3776 - val_acc: 0.8960\n",
      "Epoch 24/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0965 - acc: 0.9616 - val_loss: 0.3795 - val_acc: 0.8960\n",
      "Epoch 25/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0959 - acc: 0.9602 - val_loss: 0.3773 - val_acc: 0.8953\n",
      "Epoch 26/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0955 - acc: 0.9626 - val_loss: 0.3772 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 27/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0954 - acc: 0.9600 - val_loss: 0.3752 - val_acc: 0.8960\n",
      "Epoch 28/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0946 - acc: 0.9600 - val_loss: 0.3762 - val_acc: 0.8967\n",
      "Epoch 29/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0938 - acc: 0.9616 - val_loss: 0.3761 - val_acc: 0.8953\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 30/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0935 - acc: 0.9606 - val_loss: 0.3770 - val_acc: 0.8953\n",
      "Epoch 31/100\n",
      "5881/5881 [==============================] - 0s 34us/step - loss: 0.0920 - acc: 0.9623 - val_loss: 0.3765 - val_acc: 0.8946\n",
      "Epoch 32/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0943 - acc: 0.9638 - val_loss: 0.3778 - val_acc: 0.8953\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 33/100\n",
      "5881/5881 [==============================] - 0s 35us/step - loss: 0.0929 - acc: 0.9617 - val_loss: 0.3787 - val_acc: 0.8960\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00033: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a6070bf60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'dense'\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(model_name + '_' + str(time())))\n",
    "callbacks_dense = callbacks + [tensorboard]\n",
    "\n",
    "#generate model\n",
    "dense_input, dense_output , _ = dense_model_generator(n_timesteps, n_features)\n",
    "model = Model(dense_input, dense_output, name = model_name)\n",
    "\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, \n",
    "          y_train, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=validation_split_on_training,\n",
    "          verbose=True,\n",
    "         callbacks = callbacks_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 128, 9)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               [(None, 128), (None, 0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 20)           2580        lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 20)           2580        lambda_3[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 20)           2580        lambda_3[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 20)           2580        lambda_3[0][3]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 20)           2580        lambda_3[0][4]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 20)           2580        lambda_3[0][5]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 20)           2580        lambda_3[0][6]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 20)           2580        lambda_3[0][7]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 20)           2580        lambda_3[0][8]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 20, 9)        0           dense_14[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "                                                                 dense_16[0][0]                   \n",
      "                                                                 dense_17[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "                                                                 dense_21[0][0]                   \n",
      "                                                                 dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100)          44000       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 9)        0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100)          0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 180)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 100)          10100       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 250)          45250       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 350)          0           dense_26[0][0]                   \n",
      "                                                                 dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 6)            2106        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 124,676\n",
      "Trainable params: 124,676\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 5881 samples, validate on 1471 samples\n",
      "Epoch 1/100\n",
      "5881/5881 [==============================] - 5s 851us/step - loss: 1.4611 - acc: 0.4030 - val_loss: 1.1512 - val_acc: 0.6390\n",
      "Epoch 2/100\n",
      "5881/5881 [==============================] - 4s 646us/step - loss: 0.9612 - acc: 0.6478 - val_loss: 0.9848 - val_acc: 0.7124\n",
      "Epoch 3/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.7437 - acc: 0.7381 - val_loss: 0.8530 - val_acc: 0.7899\n",
      "Epoch 4/100\n",
      "5881/5881 [==============================] - 4s 645us/step - loss: 0.5804 - acc: 0.8145 - val_loss: 0.7434 - val_acc: 0.8232\n",
      "Epoch 5/100\n",
      "5881/5881 [==============================] - 4s 651us/step - loss: 0.4586 - acc: 0.8602 - val_loss: 0.6406 - val_acc: 0.8396\n",
      "Epoch 6/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.3574 - acc: 0.8818 - val_loss: 0.5671 - val_acc: 0.8532\n",
      "Epoch 7/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.2943 - acc: 0.9017 - val_loss: 0.5249 - val_acc: 0.8579\n",
      "Epoch 8/100\n",
      "5881/5881 [==============================] - 4s 646us/step - loss: 0.2465 - acc: 0.9163 - val_loss: 0.5015 - val_acc: 0.8790\n",
      "Epoch 9/100\n",
      "5881/5881 [==============================] - 4s 651us/step - loss: 0.2105 - acc: 0.9313 - val_loss: 0.4824 - val_acc: 0.8804\n",
      "Epoch 10/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.1854 - acc: 0.9388 - val_loss: 0.4697 - val_acc: 0.8831\n",
      "Epoch 11/100\n",
      "5881/5881 [==============================] - 4s 647us/step - loss: 0.1674 - acc: 0.9425 - val_loss: 0.4650 - val_acc: 0.8939\n",
      "Epoch 12/100\n",
      "5881/5881 [==============================] - 4s 652us/step - loss: 0.1619 - acc: 0.9454 - val_loss: 0.4525 - val_acc: 0.8960\n",
      "Epoch 13/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.1453 - acc: 0.9492 - val_loss: 0.4537 - val_acc: 0.8926\n",
      "Epoch 14/100\n",
      "5881/5881 [==============================] - 4s 649us/step - loss: 0.1316 - acc: 0.9537 - val_loss: 0.4433 - val_acc: 0.8994\n",
      "Epoch 15/100\n",
      "5881/5881 [==============================] - 4s 648us/step - loss: 0.1223 - acc: 0.9568 - val_loss: 0.4502 - val_acc: 0.8987\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 16/100\n",
      "5881/5881 [==============================] - 4s 644us/step - loss: 0.1159 - acc: 0.9585 - val_loss: 0.4592 - val_acc: 0.9001\n",
      "Epoch 17/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.1118 - acc: 0.9577 - val_loss: 0.4616 - val_acc: 0.9001\n",
      "Epoch 18/100\n",
      "5881/5881 [==============================] - 4s 648us/step - loss: 0.1097 - acc: 0.9594 - val_loss: 0.4645 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 19/100\n",
      "5881/5881 [==============================] - 4s 652us/step - loss: 0.1057 - acc: 0.9609 - val_loss: 0.4655 - val_acc: 0.9014\n",
      "Epoch 20/100\n",
      "5881/5881 [==============================] - 4s 649us/step - loss: 0.1047 - acc: 0.9587 - val_loss: 0.4648 - val_acc: 0.9007\n",
      "Epoch 21/100\n",
      "5881/5881 [==============================] - 4s 649us/step - loss: 0.1036 - acc: 0.9606 - val_loss: 0.4663 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 22/100\n",
      "5881/5881 [==============================] - 4s 650us/step - loss: 0.1019 - acc: 0.9612 - val_loss: 0.4661 - val_acc: 0.9014\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f495c6a25f8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ens'\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(model_name + '_' + str(time())))\n",
    "callbacks_ens = callbacks + [tensorboard]\n",
    "\n",
    "ens_input, ens_output, _  = hibrid_ens_generator(n_timesteps, n_features)\n",
    "\n",
    "\n",
    "model = Model(ens_input, ens_output, name= model_name)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=validation_split_on_training,\n",
    "          verbose=True,\n",
    "         callbacks = callbacks_ens)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
