{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on \n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/\n",
    "\n",
    "https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from custom_tensorboard import TrainValTensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = dstack(loaded)\n",
    "\treturn loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix)\n",
    "    #trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix)\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = load_dataset('../data/HAR/UCI_HAR_Dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 128, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Lambda, Input, Dropout, Flatten, LSTM, Concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 100)               44000     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 54,706\n",
      "Trainable params: 54,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6616 samples, validate on 736 samples\n",
      "Epoch 1/25\n",
      "6616/6616 [==============================] - 4s 651us/step - loss: 1.7790 - acc: 0.2013 - val_loss: 1.7740 - val_acc: 0.2092\n",
      "Epoch 2/25\n",
      "6616/6616 [==============================] - 3s 521us/step - loss: 1.7733 - acc: 0.2320 - val_loss: 1.7694 - val_acc: 0.3845\n",
      "Epoch 3/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 1.7708 - acc: 0.2399 - val_loss: 1.7649 - val_acc: 0.3913\n",
      "Epoch 4/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 1.7666 - acc: 0.2680 - val_loss: 1.7608 - val_acc: 0.3913\n",
      "Epoch 5/25\n",
      "6616/6616 [==============================] - 3s 518us/step - loss: 1.7640 - acc: 0.2781 - val_loss: 1.7569 - val_acc: 0.3913\n",
      "Epoch 6/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 1.7581 - acc: 0.2999 - val_loss: 1.7530 - val_acc: 0.3913\n",
      "Epoch 7/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 1.7561 - acc: 0.3191 - val_loss: 1.7491 - val_acc: 0.3913\n",
      "Epoch 8/25\n",
      "6616/6616 [==============================] - 3s 523us/step - loss: 1.7514 - acc: 0.3413 - val_loss: 1.7453 - val_acc: 0.3899\n",
      "Epoch 9/25\n",
      "6616/6616 [==============================] - 3s 521us/step - loss: 1.7486 - acc: 0.3463 - val_loss: 1.7415 - val_acc: 0.3886\n",
      "Epoch 10/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 1.7448 - acc: 0.3537 - val_loss: 1.7377 - val_acc: 0.3872\n",
      "Epoch 11/25\n",
      "6616/6616 [==============================] - 3s 521us/step - loss: 1.7400 - acc: 0.3674 - val_loss: 1.7340 - val_acc: 0.3872\n",
      "Epoch 12/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 1.7374 - acc: 0.3792 - val_loss: 1.7302 - val_acc: 0.3872\n",
      "Epoch 13/25\n",
      "6616/6616 [==============================] - 3s 524us/step - loss: 1.7335 - acc: 0.3827 - val_loss: 1.7262 - val_acc: 0.3872\n",
      "Epoch 14/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 1.7302 - acc: 0.3847 - val_loss: 1.7223 - val_acc: 0.3872\n",
      "Epoch 15/25\n",
      "6616/6616 [==============================] - 3s 515us/step - loss: 1.7246 - acc: 0.3916 - val_loss: 1.7182 - val_acc: 0.3859\n",
      "Epoch 16/25\n",
      "6616/6616 [==============================] - 3s 518us/step - loss: 1.7218 - acc: 0.3916 - val_loss: 1.7140 - val_acc: 0.3804\n",
      "Epoch 17/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 1.7163 - acc: 0.3977 - val_loss: 1.7096 - val_acc: 0.3723\n",
      "Epoch 18/25\n",
      "6616/6616 [==============================] - 3s 522us/step - loss: 1.7141 - acc: 0.3909 - val_loss: 1.7051 - val_acc: 0.3709\n",
      "Epoch 19/25\n",
      "6616/6616 [==============================] - 3s 521us/step - loss: 1.7095 - acc: 0.3983 - val_loss: 1.7004 - val_acc: 0.3709\n",
      "Epoch 20/25\n",
      "6616/6616 [==============================] - 3s 522us/step - loss: 1.7049 - acc: 0.3983 - val_loss: 1.6955 - val_acc: 0.3709\n",
      "Epoch 21/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 1.6999 - acc: 0.4016 - val_loss: 1.6905 - val_acc: 0.3709\n",
      "Epoch 22/25\n",
      "6616/6616 [==============================] - 3s 518us/step - loss: 1.6950 - acc: 0.3987 - val_loss: 1.6852 - val_acc: 0.3709\n",
      "Epoch 23/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 1.6907 - acc: 0.3960 - val_loss: 1.6798 - val_acc: 0.3709\n",
      "Epoch 24/25\n",
      "6616/6616 [==============================] - 3s 515us/step - loss: 1.6852 - acc: 0.4016 - val_loss: 1.6742 - val_acc: 0.3709\n",
      "Epoch 25/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 1.6804 - acc: 0.3986 - val_loss: 1.6684 - val_acc: 0.3709\n",
      "Train on 6616 samples, validate on 736 samples\n",
      "Epoch 1/25\n",
      "6616/6616 [==============================] - 4s 658us/step - loss: 1.7358 - acc: 0.3310 - val_loss: 1.6241 - val_acc: 0.4823\n",
      "Epoch 2/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 1.5254 - acc: 0.3891 - val_loss: 1.3758 - val_acc: 0.3723\n",
      "Epoch 3/25\n",
      "6616/6616 [==============================] - 3s 525us/step - loss: 1.3809 - acc: 0.3817 - val_loss: 1.2728 - val_acc: 0.4429\n",
      "Epoch 4/25\n",
      "6616/6616 [==============================] - 3s 521us/step - loss: 1.2812 - acc: 0.4728 - val_loss: 1.1178 - val_acc: 0.5421\n",
      "Epoch 5/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 1.1650 - acc: 0.5012 - val_loss: 1.0775 - val_acc: 0.4647\n",
      "Epoch 6/25\n",
      "6616/6616 [==============================] - 3s 518us/step - loss: 1.0743 - acc: 0.5382 - val_loss: 0.9248 - val_acc: 0.6223\n",
      "Epoch 7/25\n",
      "6616/6616 [==============================] - 3s 521us/step - loss: 1.0202 - acc: 0.5535 - val_loss: 0.8749 - val_acc: 0.6141\n",
      "Epoch 8/25\n",
      "6616/6616 [==============================] - 3s 518us/step - loss: 0.9521 - acc: 0.6066 - val_loss: 0.8301 - val_acc: 0.6277\n",
      "Epoch 9/25\n",
      "6616/6616 [==============================] - 3s 521us/step - loss: 0.8704 - acc: 0.6345 - val_loss: 0.7808 - val_acc: 0.6345\n",
      "Epoch 10/25\n",
      "6616/6616 [==============================] - 3s 518us/step - loss: 0.7868 - acc: 0.6608 - val_loss: 0.6521 - val_acc: 0.7228\n",
      "Epoch 11/25\n",
      "6616/6616 [==============================] - 3s 518us/step - loss: 0.8548 - acc: 0.6504 - val_loss: 0.6850 - val_acc: 0.7052\n",
      "Epoch 12/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 0.7117 - acc: 0.6921 - val_loss: 0.6004 - val_acc: 0.7337\n",
      "Epoch 13/25\n",
      "6616/6616 [==============================] - 3s 516us/step - loss: 0.7058 - acc: 0.7121 - val_loss: 0.6027 - val_acc: 0.7391\n",
      "Epoch 14/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 0.6815 - acc: 0.7222 - val_loss: 0.5609 - val_acc: 0.7473\n",
      "Epoch 15/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 0.6222 - acc: 0.7523 - val_loss: 0.4906 - val_acc: 0.8220\n",
      "Epoch 16/25\n",
      "6616/6616 [==============================] - 3s 523us/step - loss: 0.5863 - acc: 0.7660 - val_loss: 0.5196 - val_acc: 0.7989\n",
      "Epoch 17/25\n",
      "6616/6616 [==============================] - 3s 518us/step - loss: 0.5769 - acc: 0.7707 - val_loss: 0.4964 - val_acc: 0.7894\n",
      "Epoch 18/25\n",
      "6616/6616 [==============================] - 3s 516us/step - loss: 0.5386 - acc: 0.7887 - val_loss: 0.4557 - val_acc: 0.8071\n",
      "Epoch 19/25\n",
      "6616/6616 [==============================] - 3s 518us/step - loss: 0.5080 - acc: 0.8026 - val_loss: 0.5764 - val_acc: 0.7473\n",
      "Epoch 20/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 0.5056 - acc: 0.8009 - val_loss: 0.4966 - val_acc: 0.8057\n",
      "Epoch 21/25\n",
      "6616/6616 [==============================] - 3s 521us/step - loss: 0.4963 - acc: 0.8043 - val_loss: 0.4182 - val_acc: 0.8261\n",
      "Epoch 22/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 0.4758 - acc: 0.8099 - val_loss: 0.4534 - val_acc: 0.8193\n",
      "Epoch 23/25\n",
      "6616/6616 [==============================] - 3s 519us/step - loss: 0.4615 - acc: 0.8147 - val_loss: 0.4424 - val_acc: 0.8207\n",
      "Epoch 24/25\n",
      "6616/6616 [==============================] - 3s 520us/step - loss: 0.4356 - acc: 0.8325 - val_loss: 0.4258 - val_acc: 0.8438\n",
      "Epoch 25/25\n",
      "6616/6616 [==============================] - 3s 523us/step - loss: 0.4430 - acc: 0.8259 - val_loss: 0.3937 - val_acc: 0.8220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4f3b39470>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'LSTM'\n",
    "\n",
    "model = Sequential(name =name)\n",
    "model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(name + '_' + str(time())))\n",
    "model.fit(train_x, \n",
    "          train_y, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=0.1,\n",
    "          verbose=True,\n",
    "         callbacks = [tensorboard],\n",
    ")\n",
    "#Input\n",
    "x = Input((n_timesteps, n_features))\n",
    "\n",
    "#LSTM\n",
    "lstm_1 = LSTM(100, input_shape=(n_timesteps,n_features))(x)\n",
    "lstm_2 = Dropout(0.5)(lstm_1)\n",
    "lstm_3 = Dense(100, activation='relu')(lstm_2)\n",
    "lstm_4 = Dense(n_outputs, activation='softmax', name='lstm_out')(lstm_3)\n",
    "\n",
    "model = Model(x, lstm_4, name= 'LSTM' )\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(name + '_' + str(time())))\n",
    "model.fit(train_x, \n",
    "          train_y, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=0.1,\n",
    "          verbose=True,\n",
    "         callbacks = [tensorboard])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some comments about this architecture:\n",
    "\n",
    "- Note that the unstack dim is the feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 128, 9)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              [(None, 128), (None, 0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 20)           2580        lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_94 (Dense)                (None, 20)           2580        lambda_15[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_95 (Dense)                (None, 20)           2580        lambda_15[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_96 (Dense)                (None, 20)           2580        lambda_15[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 20)           2580        lambda_15[0][4]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 20)           2580        lambda_15[0][5]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 20)           2580        lambda_15[0][6]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 20)           2580        lambda_15[0][7]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 20)           2580        lambda_15[0][8]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 20, 9)        0           dense_93[0][0]                   \n",
      "                                                                 dense_94[0][0]                   \n",
      "                                                                 dense_95[0][0]                   \n",
      "                                                                 dense_96[0][0]                   \n",
      "                                                                 dense_97[0][0]                   \n",
      "                                                                 dense_98[0][0]                   \n",
      "                                                                 dense_99[0][0]                   \n",
      "                                                                 dense_100[0][0]                  \n",
      "                                                                 dense_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 20, 9)        0           lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 180)          0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_102 (Dense)               (None, 250)          45250       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 20)           5020        dense_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_104 (Dense)               (None, 6)            126         dense_103[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 73,616\n",
      "Trainable params: 73,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 6616 samples, validate on 736 samples\n",
      "Epoch 1/25\n",
      "6616/6616 [==============================] - 1s 131us/step - loss: 1.5070 - acc: 0.4581 - val_loss: 1.1936 - val_acc: 0.5897\n",
      "Epoch 2/25\n",
      "6616/6616 [==============================] - 0s 35us/step - loss: 1.1210 - acc: 0.5967 - val_loss: 0.9257 - val_acc: 0.6481\n",
      "Epoch 3/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.9116 - acc: 0.6528 - val_loss: 0.7400 - val_acc: 0.6943\n",
      "Epoch 4/25\n",
      "6616/6616 [==============================] - 0s 35us/step - loss: 0.7513 - acc: 0.7118 - val_loss: 0.5931 - val_acc: 0.7976\n",
      "Epoch 5/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.6068 - acc: 0.7931 - val_loss: 0.4846 - val_acc: 0.8207\n",
      "Epoch 6/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.5050 - acc: 0.8346 - val_loss: 0.4108 - val_acc: 0.8438\n",
      "Epoch 7/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.4227 - acc: 0.8605 - val_loss: 0.3671 - val_acc: 0.8424\n",
      "Epoch 8/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.3588 - acc: 0.8848 - val_loss: 0.3326 - val_acc: 0.8546\n",
      "Epoch 9/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.3103 - acc: 0.9001 - val_loss: 0.2875 - val_acc: 0.8655\n",
      "Epoch 10/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.2720 - acc: 0.9054 - val_loss: 0.2862 - val_acc: 0.8859\n",
      "Epoch 11/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.2438 - acc: 0.9211 - val_loss: 0.2744 - val_acc: 0.8641\n",
      "Epoch 12/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.2152 - acc: 0.9259 - val_loss: 0.2750 - val_acc: 0.8899\n",
      "Epoch 13/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1964 - acc: 0.9377 - val_loss: 0.2596 - val_acc: 0.8750\n",
      "Epoch 14/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1839 - acc: 0.9356 - val_loss: 0.2548 - val_acc: 0.8995\n",
      "Epoch 15/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1706 - acc: 0.9444 - val_loss: 0.2492 - val_acc: 0.8859\n",
      "Epoch 16/25\n",
      "6616/6616 [==============================] - 0s 35us/step - loss: 0.1581 - acc: 0.9454 - val_loss: 0.2374 - val_acc: 0.8940\n",
      "Epoch 17/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1514 - acc: 0.9488 - val_loss: 0.2300 - val_acc: 0.8913\n",
      "Epoch 18/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1437 - acc: 0.9491 - val_loss: 0.2542 - val_acc: 0.9022\n",
      "Epoch 19/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1409 - acc: 0.9492 - val_loss: 0.2091 - val_acc: 0.8954\n",
      "Epoch 20/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1338 - acc: 0.9524 - val_loss: 0.2405 - val_acc: 0.9062\n",
      "Epoch 21/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1276 - acc: 0.9531 - val_loss: 0.2298 - val_acc: 0.9049\n",
      "Epoch 22/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1257 - acc: 0.9539 - val_loss: 0.2355 - val_acc: 0.9103\n",
      "Epoch 23/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1193 - acc: 0.9568 - val_loss: 0.2294 - val_acc: 0.9049\n",
      "Epoch 24/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1190 - acc: 0.9551 - val_loss: 0.2128 - val_acc: 0.8927\n",
      "Epoch 25/25\n",
      "6616/6616 [==============================] - 0s 34us/step - loss: 0.1165 - acc: 0.9530 - val_loss: 0.2496 - val_acc: 0.9090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4f3366320>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'dense'\n",
    "\n",
    "#Input\n",
    "x = Input((n_timesteps, n_features))\n",
    "#Dense\n",
    "dense_1 = Lambda(lambda x: K.tf.unstack(x, axis=2))(x)\n",
    "dense_2 = [Dense(20)(x) for x in dense_1]\n",
    "dense_3 = Lambda(lambda x: K.stack(x, axis=2))(dense_2)\n",
    "dense_4 = Dropout(0.1)(dense_3)\n",
    "dense_5 = Flatten()(dense_4)\n",
    "dense_6 = Dense(250, activation = 'relu')(dense_5)\n",
    "dense_7 = Dense(20, activation = 'relu')(dense_6)\n",
    "dense_8 = Dense(n_outputs, activation='softmax')(dense_7)\n",
    "\n",
    "\n",
    "model = Model(x, dense_8, name ='dense')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(name + '_' + str(time())))\n",
    "model.fit(train_x, \n",
    "          train_y, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=0.1,\n",
    "          verbose=True,\n",
    "         callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ens'\n",
    "\n",
    "#Input\n",
    "x = Input((n_timesteps, n_features))\n",
    "\n",
    "#LSTM\n",
    "lstm_1 = LSTM(100, input_shape=(n_timesteps,n_features))(x)\n",
    "lstm_2 = Dropout(0.5)(lstm_1)\n",
    "lstm_3 = Dense(100, activation='relu')(lstm_2)\n",
    "lstm_4 = Dense(n_outputs, activation='softmax', name='lstm_out')(lstm_3)\n",
    "#Dense\n",
    "dense_1 = Lambda(lambda x: K.tf.unstack(x, axis=2))(x)\n",
    "dense_2 = [Dense(20)(x) for x in dense_1]\n",
    "dense_3 = Lambda(lambda x: K.stack(x, axis=2))(dense_2)\n",
    "dense_4 = Dropout(0.1)(dense_3)\n",
    "dense_5 = Flatten()(dense_4)\n",
    "dense_6 = Dense(250, activation = 'relu')(dense_5)\n",
    "dense_7 = Dense(20, activation = 'relu')(dense_6)\n",
    "dense_8 = Dense(n_outputs, activation='softmax')(dense_7)\n",
    "\n",
    "ens_1 = Concatenate(axis=1)([lstm_3,dense_7])\n",
    "ens_2 = Dense(n_outputs, activation='softmax')(ens_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, ens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6616 samples, validate on 736 samples\n",
      "Epoch 1/25\n",
      "6616/6616 [==============================] - 5s 758us/step - loss: 1.5578 - acc: 0.4022 - val_loss: 1.2925 - val_acc: 0.5557\n",
      "Epoch 2/25\n",
      "6616/6616 [==============================] - 3s 524us/step - loss: 1.2221 - acc: 0.5289 - val_loss: 1.0492 - val_acc: 0.5815\n",
      "Epoch 3/25\n",
      "6616/6616 [==============================] - 3s 526us/step - loss: 1.0223 - acc: 0.5859 - val_loss: 0.8518 - val_acc: 0.6807\n",
      "Epoch 4/25\n",
      "6616/6616 [==============================] - 3s 524us/step - loss: 0.9265 - acc: 0.6375 - val_loss: 0.7999 - val_acc: 0.6440\n",
      "Epoch 5/25\n",
      "6616/6616 [==============================] - 3s 524us/step - loss: 0.8255 - acc: 0.6911 - val_loss: 0.7274 - val_acc: 0.7120\n",
      "Epoch 6/25\n",
      "6616/6616 [==============================] - 3s 527us/step - loss: 0.7065 - acc: 0.7406 - val_loss: 0.6016 - val_acc: 0.7405\n",
      "Epoch 7/25\n",
      "4000/6616 [=================>............] - ETA: 1s - loss: 0.6236 - acc: 0.7823"
     ]
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(name + '_' + str(time())))\n",
    "\n",
    "model.fit(train_x, \n",
    "          train_y, epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=0.1,\n",
    "          verbose=True,\n",
    "         callbacks = [tensorboard])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
